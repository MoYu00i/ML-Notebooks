{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvPdSz11yT-t",
        "outputId": "059e850c-2561-4d95-92eb-5f0e4876c7c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3.], grad_fn=<AddBackward0>)\n",
            "tensor([2.], grad_fn=<AddBackward0>)\n",
            "tensor([6.], grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# PyTorch提供了autograd包来自动根据输入和前向传播构建计算图，其中，backward函数可以很轻松的计算出梯度\n",
        "# Tensor在pytorch中用来表示张量，且均为直接被我们创建的。如果我们想使用autograd包让它们参与梯度计算，则需要在创建它们的时候，将.requires_grad属性指定为true。\n",
        "# 在梯度的反向计算过程中，只有叶子节点的梯度才会被填充。对于非叶子节点，如果要填充梯度信息，需要显式设置retain_grad=true。\n",
        "a = torch.tensor([2.], requires_grad=True) # 定义a,b两个张量 requires_grad=True 的作用是让 backward 可以追踪这个参数并且计算它的梯度。\n",
        "b = torch.tensor([1.], requires_grad=True)\n",
        "\n",
        "c = a + b\n",
        "d = b + 1\n",
        "e = c * d\n",
        "\n",
        "#为非叶节点填充的梯度\n",
        "#在默认情况下，我们只能计算叶节点的导数值\n",
        "# 中间节点的梯度值不会被保存，若想保存中间节点的梯度，我们可以使用retain_grad方法\n",
        "c.retain_grad() # 通过.retain_grad()保留非任意节点的梯度值\n",
        "d.retain_grad()\n",
        "e.retain_grad()\n",
        "\n",
        "print(c)\n",
        "print(d)\n",
        "print(e) #输出e的值"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e.backward() #使用backward方法执行反向传播"
      ],
      "metadata": {
        "id": "TTwGVY2Nfmv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 张量的grad属性存储了该点处的导数值。\n",
        "print(a.grad) #a的梯度为2*1"
      ],
      "metadata": {
        "id": "2T0W9OsYgxTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(b.grad) #b的梯度为1*2+1*3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVibrCO8kYq0",
        "outputId": "05199630-526d-43e6-e697-c7cab77b81a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.grad, b.grad, c.grad, d.grad, e.grad) #c的梯度为2，d的梯度为5，e的梯度为1"
      ],
      "metadata": {
        "id": "dgIQJHorlXsZ",
        "outputId": "f2bd5a55-dd20-43b3-d309-679b795022c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.]) tensor([5.]) tensor([2.]) tensor([3.]) tensor([1.])\n"
          ]
        }
      ]
    }
  ]
}